{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PROJECT: Kaggle Playground Series - S6E2 (Heart Disease)\n",
        "# MISSION: Establishing the Baseline Model & Overcoming Encoding Hurdles\n",
        "# NARRATOR: [Your Name]\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# --- STEP 1: MOUNTING THE ARMORY (Google Drive) ---\n",
        "# Connecting to Google Drive to access the competition datasets.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- STEP 2: LOADING THE DATA ---\n",
        "# Replace these strings with your actual Google Drive file paths.\n",
        "TRAIN_PATH = '/content/drive/MyDrive/Nihal Data/kaggle/S6E1 - heart/train.csv'\n",
        "TEST_PATH = '/content/drive/MyDrive/Nihal Data/kaggle/S6E1 - heart/test.csv'\n",
        "\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "test = pd.read_csv(TEST_PATH)\n",
        "\n",
        "print(f\"âœ… Data Loaded! Training set size: {train.shape}\")\n",
        "\n",
        "# --- STEP 3: PREPARING THE BATTLEFIELD (Encoding) ---\n",
        "\n",
        "# 1. Target Encoding: Converting 'Absence'/'Presence' to 0/1\n",
        "# XGBoost requires numeric targets for classification.\n",
        "target_mapping = {'Absence': 0, 'Presence': 1}\n",
        "y = train['Heart Disease'].map(target_mapping)\n",
        "\n",
        "# 2. Feature Selection\n",
        "# We exclude 'id' (not a predictor) and the original 'Heart Disease' column.\n",
        "features = [col for col in train.columns if col not in ['id', 'Heart Disease']]\n",
        "X = train[features].copy()\n",
        "X_test = test[features].copy()\n",
        "\n",
        "# 3. Categorical Encoding: Handling text data in the features\n",
        "# We iterate through 'object' columns and convert them to numeric category codes.\n",
        "for col in X.select_dtypes(include=['object']).columns:\n",
        "    X[col] = X[col].astype('category').cat.codes\n",
        "    X_test[col] = X_test[col].astype('category').cat.codes\n",
        "\n",
        "print(\"âœ… Features and Target successfully encoded.\")\n",
        "\n",
        "# --- STEP 4: TRAINING THE CHAMPION (XGBoost) ---\n",
        "\n",
        "# Splitting data: 80% Training, 20% Validation\n",
        "# 'stratify=y' ensures both sets have the same percentage of Heart Disease cases.\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initializing the XGBoost Classifier\n",
        "model = XGBClassifier(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    tree_method='hist', # Efficient for large tabular datasets\n",
        "    eval_metric='auc',\n",
        "    early_stopping_rounds=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fitting the model with the validation set to monitor for overfitting\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    verbose=100\n",
        ")\n",
        "\n",
        "# --- STEP 5: THE MOMENT OF TRUTH ---\n",
        "\n",
        "# Predicting probabilities for the validation set\n",
        "val_probs = model.predict_proba(X_val)[:, 1]\n",
        "final_auc = roc_auc_score(y_val, val_probs)\n",
        "\n",
        "print(f\"\\nðŸš€ MISSION SUCCESS! Local Validation AUC: {final_auc:.5f}\")\n",
        "\n",
        "# --- STEP 6: GENERATING THE SUBMISSION ---\n",
        "\n",
        "# Generating the final probability predictions for the Kaggle test set\n",
        "test_probs = model.predict_proba(X_test)[:, 1]\n",
        "submission = pd.DataFrame({'id': test['id'], 'Heart Disease': test_probs})\n",
        "\n",
        "# Saving the output to a CSV file\n",
        "submission.to_csv('submission_baseline.csv', index=False)\n",
        "\n",
        "print(\"ðŸ’¾ Submission file saved as 'submission_baseline.csv'.\")"
      ],
      "metadata": {
        "id": "ExC7ebmdXGFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8d439a-5622-498b-8a75-bbde155111ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… Data Loaded! Training set size: (630000, 15)\n",
            "âœ… Features and Target successfully encoded.\n",
            "[0]\tvalidation_0-auc:0.93991\n",
            "[100]\tvalidation_0-auc:0.95456\n",
            "[200]\tvalidation_0-auc:0.95534\n",
            "[300]\tvalidation_0-auc:0.95577\n",
            "[400]\tvalidation_0-auc:0.95592\n",
            "[500]\tvalidation_0-auc:0.95596\n",
            "[600]\tvalidation_0-auc:0.95597\n",
            "[615]\tvalidation_0-auc:0.95597\n",
            "\n",
            "ðŸš€ MISSION SUCCESS! Local Validation AUC: 0.95597\n",
            "ðŸ’¾ Submission file saved as 'submission_baseline.csv'.\n"
          ]
        }
      ]
    }
  ]
}
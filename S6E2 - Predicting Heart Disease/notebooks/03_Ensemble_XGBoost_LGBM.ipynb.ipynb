{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PROJECT: Kaggle Playground Series - S6E2 (Heart Disease)\n",
        "# MISSION: The Power Ensemble (XGBoost + LightGBM)\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# --- STEP 1: MOUNT & LOAD ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "TRAIN_PATH = '/content/drive/MyDrive/Nihal Data/kaggle/S6E1 - heart/train.csv'\n",
        "TEST_PATH = '/content/drive/MyDrive/Nihal Data/kaggle/S6E1 - heart/test.csv'\n",
        "\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "test = pd.read_csv(TEST_PATH)\n",
        "\n",
        "print(f\" Data Loaded. Training shape: {train.shape}\")\n",
        "\n",
        "# --- STEP 2: ENCODING ---\n",
        "# Converting 'Absence'/'Presence' to binary integers\n",
        "target_mapping = {'Absence': 0, 'Presence': 1}\n",
        "y = train['Heart Disease'].map(target_mapping)\n",
        "\n",
        "features = [col for col in train.columns if col not in ['id', 'Heart Disease']]\n",
        "X = train[features].copy()\n",
        "X_test = test[features].copy()\n",
        "\n",
        "# Simple categorical encoding for string/object columns\n",
        "for col in X.select_dtypes(include=['object']).columns:\n",
        "    X[col] = X[col].astype('category').cat.codes\n",
        "    X_test[col] = X_test[col].astype('category').cat.codes\n",
        "\n",
        "# --- STEP 3: THE ENSEMBLE CROSS-VALIDATION ---\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Storage for our predictions\n",
        "final_test_preds = np.zeros(len(X_test))\n",
        "cv_scores = []\n",
        "\n",
        "print(\" Starting the Multi-Model Ensemble (XGBoost + LightGBM)...\")\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    # --- MODEL A: XGBoost (Updated for CUDA/GPU) ---\n",
        "    xgb = XGBClassifier(\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.03,\n",
        "        max_depth=7,\n",
        "        tree_method='hist',   # Use 'hist' instead of 'gpu_hist'\n",
        "        device='cuda',        # Modern way to trigger GPU in Colab\n",
        "        eval_metric='auc',\n",
        "        early_stopping_rounds=50,\n",
        "        random_state=42\n",
        "    )\n",
        "    xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=0)\n",
        "    xgb_probs = xgb.predict_proba(X_val)[:, 1]\n",
        "\n",
        "    # --- MODEL B: LightGBM ---\n",
        "    # Using 'gpu' as device; if it fails, Colab's default CPU is still very fast.\n",
        "    lgb = LGBMClassifier(\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.03,\n",
        "        num_leaves=63,\n",
        "        device='gpu',        # Uses GPU for LightGBM\n",
        "        metric='auc',\n",
        "        importance_type='gain',\n",
        "        random_state=42,\n",
        "        verbosity=-1\n",
        "    )\n",
        "    lgb.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
        "    lgb_probs = lgb.predict_proba(X_val)[:, 1]\n",
        "\n",
        "    # --- BLENDING THE TWO ---\n",
        "    # We use a 50/50 weighted average to combine their \"opinions\"\n",
        "    combined_val_probs = (0.5 * xgb_probs) + (0.5 * lgb_probs)\n",
        "    score = roc_auc_score(y_val, combined_val_probs)\n",
        "    cv_scores.append(score)\n",
        "\n",
        "    # Predict on test set for this fold\n",
        "    fold_test_preds = (0.5 * xgb.predict_proba(X_test)[:, 1]) + (0.5 * lgb.predict_proba(X_test)[:, 1])\n",
        "    final_test_preds += fold_test_preds / 5 # Average across 5 folds\n",
        "\n",
        "    print(f\" Fold {fold+1} Ensemble AUC: {score:.5f}\")\n",
        "\n",
        "print(f\"\\n Mean Ensemble CV AUC: {np.mean(cv_scores):.5f}\")\n",
        "\n",
        "# --- STEP 4: SUBMISSION ---\n",
        "submission = pd.DataFrame({'id': test['id'], 'Heart Disease': final_test_preds})\n",
        "submission.to_csv('submission_v3_ensemble.csv', index=False)\n",
        "print(\" 'submission_v3_ensemble.csv' has been generated and is ready for Kaggle!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nptcnMuTsx4C",
        "outputId": "ce9b1cd6-09eb-4249-b743-f37ee052c6c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " Data Loaded. Training shape: (630000, 15)\n",
            " Starting the Multi-Model Ensemble (XGBoost + LightGBM)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [07:56:29] WARNING: /workspace/src/common/error_msg.cc:62: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "Potential solutions:\n",
            "- Use a data structure that matches the device ordinal in the booster.\n",
            "- Set the device for booster before call to inplace_predict.\n",
            "\n",
            "This warning will only be shown once.\n",
            "\n",
            "  return func(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 1 Ensemble AUC: 0.95534\n",
            " Fold 2 Ensemble AUC: 0.95442\n",
            " Fold 3 Ensemble AUC: 0.95522\n",
            " Fold 4 Ensemble AUC: 0.95470\n",
            " Fold 5 Ensemble AUC: 0.95553\n",
            "\n",
            " Mean Ensemble CV AUC: 0.95504\n",
            " 'submission_v3_ensemble.csv' has been generated and is ready for Kaggle!\n"
          ]
        }
      ]
    }
  ]
}